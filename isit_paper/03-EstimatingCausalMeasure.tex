\section{Estimation of the Causal Measure} \label{estimation}

\begin{comment}
- Sequential Prediction
- Notion of causality regret - whereas DI has one value that the Tsachy paper tries to converge to, we are looking more at a sequential prediction problem because we want to estimate causality accurately at every point in time
- Theorem
- Independent selection of restricted and complete reference classes - addresses problem with restricted distribution being infinite order raised in Purdon paper
- Discussion of estimating with time varying statistics
- Can be computed online
\end{comment}

Assuming the joint statistics underlying the observed data are unknown, it is necessary to develop methods for estimating the causal measure. As such, an estimate of the causal measure can be obtained by simply estimating the complete and restricted distributions and then computing the KL divergence between the two at each time. Such an estimator allows us to leverage results from the field of sequential prediction.

The sequential prediction problem formulation we consider is as follows: A learner is sequentially observing a sequence $x_1,x_2,\dots,x_n$ over some space of observations $\mc{X}$. At each round, $i$, having observed the sequence $x_1,\dots,x_{i-1}$ (or more generally, the history $\history{i}$, which may include information in addition to $x^{i-1}$), the learner selects a probability assignment $\hat{f}_i \in \mc{P}$, where $\mc{P}$ is the space of probability distributions over $\mc{X}$. Once $\hat{f}_i$ is chosen, $x_i$ is revealed and a loss $l(\hat{f}_i,x_i)$ is incurred by the learner, where the loss function $l:\mc{X}\rightarrow \mathbb{R}$ is chosen to be the self-information loss given by:

\begin{equation}
l(f,x) = -\log(f(x))
\end{equation}

The performance of sequential predictors is typically assessed using a notion of \emph{regret} with respect to a reference class of probability distributions $\refclass \subset \mc{P}$. For a given round $i$ and reference distribution $\tilde{f}_i \in \refclass$, the learner's regret is given by the difference in loss of the chosen probability assignment and the loss of the reference distribution:

\begin{equation}
r(\tilde{f}_i,x_i) = l(\hat{f}_i,x_i) - l(\tilde{f}_i,x_i)
\end{equation}

\noindent In many cases the performance of sequential predictors will be measured by the worst case regret, given by:

\begin{align}
R_n(\refclass_n) &= \sup_{x^n \in \mc{X}^n} \sum_{i=1}^n l(\hat{f}_i,x_i) - \inf_{\tilde{f}\in \refclass_n} \sum_{i=1}^n l(\tilde{f}_i,x_i) \label{optimal_f} \\
&\triangleq \sup_{x^n \in \mc{X}^n} \sum_{i=1}^n r(f^*_i,x_i)
\end{align}

\noindent where $f^*_i \in \refclass$ is defined as the distribution from the reference class with the smallest cumulative loss up to time $n$. We also define $f^* \in \refclass_n \subset \mc{P}^n$ to be the cumulative loss minimizing \emph{joint} distribution, noting that the reference class of joint distributions $\refclass_n$ is not necessarily equal to $\refclass^n$ (i.e. $\refclass \times \refclass \times \dots$), as often times there may be a constraint on the selection of the best reference distribution that is imposed in order to establish bounds. In the absence of any restrictions, the reference distributions may be selected at each time such that $f^*_i(x_i)=1$, resulting in zero cumulative loss for any sequence $x^n$. Thus, bounds on regret often assume stationarity by enforcing $f_1^*=f_2^*=\dots=f_n^*$ or assume that $f_i^* = f^*_{i+1}$ for all but some small number of indices. For various learning algorithms (i.e. strategies for selecting $\hat{f}_i$ given $\history{i}$) and reference classes $\refclass_n$, these bounds on the worst case regret are established as a function of the sequence length $n$:

\begin{equation}
R_n(\refclass_n) \le M(n)
\end{equation}

It follows naturally that an estimator for our causal measure can be constructed by building two sequential predictors. The restricted predictor $\estfxr{i}$ computed at each round using $\hr{i} \triangleq \{x_1,\dots,x_{i-1}\} \cup \{z_1,\dots,z_{i-1}\}$, and the complete predictor $\estfxc{i}$ computed at each round using $\hc{i} \triangleq \{x_1,\dots,x_{i-1}\} \cup \{y_1,\dots,y_{i-1}\} \cup \{z_1,\dots,z_{i-1}\}$. It then follows that each of these predictors will have an associated worst case regret, given by $R^{(r)}_n(\refclassr_n)$ and $R^{(c)}_n(\refclassc_n)$, where $\refclassr_n$ and $\refclassc_n$ represent the restricted and complete reference classes. Additionally we will define the instantaneous regrets for each of our predictors as $r^{(r)}$ and $r^{(c)}$. Using these sequential predictors, we define our estimated causal influence from $Y$ to $X$ at time $n$ as:

\begin{equation}
\estcyx(i) = \kl{\estfxc{i}}{\estfxr{i}}
\end{equation}

To assess the performance of an estimate of the causal measure, we define a notion of causality regret:

\begin{equation}
CR(n) \triangleq \sum_{i=1}^n \left| \estcyx(i) - \optcyx(i)  \right|
\end{equation}

\noindent where we define:

\begin{equation}
\optcyx(i) = \kl{\optfxc{i}}{\optfxr{i}}
\end{equation}

\noindent with $\optfxc{i} \in \refclass^{(c)}$ and $\optfxr{i} \in \refclass^{(r)}$ defined as the loss minimizing distributions from the complete and restricted reference classes. We note that with this notion of causal regret, the estimated causal measure is being compared against the best estimate of the causal measure from within a reference class. As such, we limit our consideration to the scenario in which the reference classes are sufficiently representative of the true sequences to produce a desirable $\optcyx$ (i.e. $\optcyx(i) \approx \cyx(i)$ for all $i$).

We now present the necessary preliminaries for proving a finite sample bound on the estimates of causality regret for the special case when $\mc{X}$ is a discrete space. We begin by introducing two assumptions that requires the underlying sequence, the complete and restricted sequential predictors, and the loss minimizing distributions to be well-behaved in an appropriate sense.

\begin{assumption} \label{assumption:gbound}
For sequential predictors \estfxc{i} and \estfxr{i}, we assume that the collection of observations is such that:
\begin{equation}
\sup_{x \in \mc{X}} \log \frac{\estfxc{i}(x)}{\estfxr{i}(x)} < L \ \ \forall i=1,\dots,n
\end{equation}
\end{assumption}

\begin{assumption} \label{assumption:referencekl}
For loss minimizing distributions $\optfxc{i} \in \refclass^{(c)}$ and $\optfxr{i} \in \refclass^{(r)}$ and restricted sequential predictor \estfxr{i}, assume that for all $i$:
\begin{equation}
\kl{\optfxc{i}}{\optfxr{i}} \le \kl{\optfxc{i}}{\estfxr{i}}
\end{equation}
\end{assumption}

We now show that the cumulative KL divergence from the best reference distribution to the predicted distribution is less than the predictor's worst-case regret.

\begin{lemma}\label{lemma:kl}
For a sequential predictor $\hat{f}_i$ with worst case regret $M(n)$, a collection observations $(x^n,y^n,z^n)$, and any distribution from the reference class $f \in \refclass_n$:

\begin{equation}
\sum_{i=1}^n \kl{f_i}{\hat{f}_i}\le M(n)
\end{equation}
\end{lemma}

\begin{proof}
\begin{align}
\sum_{i=1}^n \kl{f_i}{\hat{f}_i}
&= \sum_{i=1}^n \sum_{x\in\mc{X}} f_i(x)
    \log \frac{f_i(x)}{\hat{f}_i(x)} \\
&\le \sum_{i=1}^n
    \left[ \sup_{x\in\mc{X}}
    \log \frac{f_i(x)}{\hat{f}_i(x)} \right]
    \sum_{x\in\mc{X}} f_i(x) \\
&= \sum_{i=1}^n \sup_{x\in\mc{X}} r(f_i,x) \\
&\le \sup_{x^n\in\mc{X}^n} \sum_{i=1}^n r(f_i,x_i)\\
&\le \sup_{x^n\in\mc{X}^n} \sup_{f\in\refclass_n} \sum_{i=1}^n r(f_i,x_i)\\
&\le M(n)
\end{align}
\end{proof}

Next, we bound the cumulative difference in expectation of a bounded function between the best reference distribution and sequential predictor.

\begin{lemma} \label{lemma:g_func}
For a sequential predictor $\hat{f}_i$ with worst case regret $M(n)\ge 1$, a collection observations $(x^n,y^n,z^n)$, cumulative loss minimizing distribution $f^*_i$, and bounded functions $g_i:\mc{X}\rightarrow [-K,K]$ with $K\in\mathbb{R}$:

\begin{equation}
\sum_{i=1}^n \left| E_{f^*_i}[g_i(X)] -
    E_{\hat{f}_i}[g_i(X)] \right| \le
    \frac{\left|\mc{X}\right|K}{\sqrt{2}}\sqrt{n \cdot M(n)}
\end{equation}
\end{lemma}

\begin{proof}
\begin{align}
\sum_{i=1}^n &\left| E_{f^*_i} [g_i(X)] -
    E_{\hat{f}_i}[g_i(X)] \right| \\
&= \sum_{i=1}^n \left| \sum_{x\in\mc{X}}
    \left[ f^*_i(x) - \hat{f}_i(x) \right] g_i(x) \right| \\
&\le \sum_{i=1}^n \sum_{x\in\mc{X}} \left| f^*_i(x) -
    \hat{f}_i(x) \right| \left| g_i(x) \right|
    \label{ref_triangle_eq}\\
&\le \sum_{i=1}^n \sum_{x\in\mc{X}}
    L\sqrt{\frac{1}{2}\kl{f^*_i}{\hat{f}_i}}
    \label{ref_pinsker_assumption}\\
&= \frac{\left|\mc{X}\right| K}{\sqrt{2}} \sum_{i=1}^n
    \sqrt{\kl{f^*_i}{\hat{f}_i}}
\end{align}

\noindent where \eqref{ref_triangle_eq} uses the triangle inequality and \eqref{ref_pinsker_assumption} uses Pinsker's inequality and the boundedness of $g_i$. Focusing now on the sum, we define $\vec{v}$ such that $\vec{v}_i = \sqrt{\kl{f^*_i}{\hat{f}_i}}$ for $i=1,\dots,n$:

\begin{align}
\sum_{i=1}^n \sqrt{\kl{f^*_i}{\hat{f}_i}}
&= \left|\left|\vec{v}\right|\right|_1 \\
&\le \sqrt{n}\left|\left|\vec{v}\right|\right|_2 \label{holders}\\
&= \sqrt{n}\left( \sum_{i=1}^n \kl{f^*_i}{\hat{f}_i}
    \right)^{\frac{1}{2}} \\
&\le \sqrt{n \cdot M(n)} \label{ref_kl_lemma}
\end{align}

\noindent where \eqref{holders} uses H\"{o}lders inequality and \eqref{ref_kl_lemma} uses Lemma \ref{lemma:kl} and the assumption that $M(n) \ge 1$.
\end{proof}

Finally, we can utilize the assumption and lemmas to bound the cumulative causality regret:

\begin{theorem}
Let the worst case regret for the predictors $\estfxr{i}$ and $\estfxc{i}$ be bounded by $R^{(r)}_n(\refclassr_n) \le M^{(r)}(n)$ and $R^{(c)}_n(\refclassc_n) \le M^{(c)}(n)$, respectively. Then, for any collection of observations $(x^n,y^n,z^n)\in \mc{X}^n \times \mc{Y}^n \times \mc{Z}^n$ such that Assumption \ref{assumption:gbound} holds with bound $L$, we have:

\begin{equation} \label{causal_bound}
\begin{aligned}
\sum_{i=1}^n & \left| \estcyx(i) - \optcyx(i) \right| \le \\
&M^{(r)}(n) + M^{(c)}(n) +
    \frac{\left|\mc{X}\right|L}{\sqrt{2}}\sqrt{n \cdot M^{(c)}(n)}
\end{aligned}
\end{equation}
\end{theorem}

\begin{proof}
We begin by defining the functions:

\begin{equation}
\hat{g}_i(X) \triangleq \log \frac{\estfxc{i}(X)}{\estfxr{i}(X)} \ \ \ \ \
g^*_i(X) \triangleq \log \frac{\optfxc{i}(X)}{\optfxr{i}(X)}.
\end{equation}

\noindent Using the definition of the causal measure and KL-divergence:

\begin{align}
&\sum_{i=1}^n \left| \estcyx(i) - \optcyx(i) \right|
\nonumber \\
& \ \ \ \ \ \ \ \ \ \ - \left|
E_{\optfxc{i}}\left[ \hat{g}_i(X)\right] -
E_{\estfxc{i}}\left[ \hat{g}_i(X)\right]
\right| \label{beginning} \\
&= \sum_{i=1}^n
\left|
E_{\optfxc{i}}\left[ g^*_i(X)\right] -
E_{\estfxc{i}}\left[ \hat{g}_i(X)\right]
\right| \nonumber \\
& \ \ \ \ \ \ \ \ \ \ - \left|
E_{\optfxc{i}}\left[ \hat{g}_i(X)\right] -
E_{\estfxc{i}}\left[ \hat{g}_i(X)\right]
\right| \\
&= \sum_{i=1}^n
\bigg| \left|
E_{\optfxc{i}}\left[ g^*_i(X)\right] -
E_{\estfxc{i}}\left[ \hat{g}_i(X)\right]
\right| \nonumber \\
& \ \ \ \ \ \ \ \ \ \ - \left|
E_{\optfxc{i}}\left[ \hat{g}_i(X)\right] -
E_{\estfxc{i}}\left[ \hat{g}_i(X)\right]
\right| \bigg| \\
&\le \sum_{i=1}^n
\bigg|
E_{\optfxc{i}}\left[ g^*_i(X)\right] -
E_{\estfxc{i}}\left[ \hat{g}_i(X)\right] \nonumber \\
& \ \ \ \ \ \ \ \ \ \ -
E_{\optfxc{i}}\left[ \hat{g}_i(X)\right] +
E_{\estfxc{i}}\left[ \hat{g}_i(X)\right] \bigg| \label{rev_tri} \\
&= \sum_{i=1}^n \left|
E_{\optfxc{i}}\left[ g^*_i(X) - \hat{g}_i(X)\right] \right| \\
&= \sum_{i=1}^n \left|
E_{\optfxc{i}}\left[
\log \frac{\optfxc{i}(X)}{\estfxc{i}(X)}
- \log \frac{\optfxr{i}(X)}{\estfxr{i}(X)}
\right] \right| \\
&\le \sum_{i=1}^n \left|
\kl{\optfxc{i}}{\estfxc{i}} \right| \nonumber \\
& \ \ \ \ \ \ \ \ \ \ +
\left|
\kl{\optfxc{i}}{\estfxr{i}} -
\kl{\optfxc{i}}{\optfxr{i}}
\right| \label{tri} \\
&\le \sum_{i=1}^n
\kl{\optfxc{i}}{\estfxc{i}} +
\kl{\optfxc{i}}{\estfxr{i}} \label{assumption2} \\
&\le M^{(c)}(n) + M^{(r)}(n) \label{regretlemma}
\end{align}

\noindent where \eqref{rev_tri} follows from the reverse triangle inequality, \eqref{tri} follows from the triangle inequality, \eqref{assumption2} follows from Assumption \ref{assumption:referencekl} and non-negativity of the KL-divergence, and \eqref{regretlemma} follows from Lemma \ref{lemma:kl}. Moving the second term of \eqref{beginning} to the other side of the inequality yields:

\begin{align}
\sum_{i=1}^n & \left| \estcyx(i) - \optcyx(i) \right| \nonumber \\
& \le M^{(c)}(n) + M^{(r)}(n) + \left|
E_{\optfxc{i}}\left[ \hat{g}_i(X)\right] -
E_{\estfxc{i}}\left[ \hat{g}_i(X)\right]
\right| \nonumber \\
& \le M^{(c)}(n) + M^{(r)}(n) +
\frac{\left|\mc{X}\right|L}{\sqrt{2}}\sqrt{n \cdot M^{(c)}(n)}
\label{final_eq}
\end{align}

\noindent where \eqref{final_eq} follows from Assumption \ref{assumption:gbound} and Lemma \ref{lemma:g_func}. This concludes the proof.

\end{proof}