\section{Introduction}

\begin{comment}
- Granger Causality
- Directed Information
- Transfer Entropy
- Local Transfer Entropy
- Sequential Prediction
- Argument for why sample path causality is important. Perhaps a neuroscientific argument that even if the model is time invariant, there may be certain patterns that induce a greater level of causal influence.
\end{itemize}}
\end{comment}

In 1969, Granger \cite{granger1969investigating} built upon the the ideas of Wiener by proposing an approach to identify causal relationships between time series. While his original treatment was applied only to linear regression models, his underlying perspective that a time series $Y^n$ is ``causing'' $X^n$ if we can better predict $Y^n$ given all information than given all information excluding $X^n$ is still utilized throughout causality research. More modern information theoretic interpretations of this principle include directed information (DI) \cite{marko1973bidirectional,massey1990causality} and transfer entropy (TE) \cite{schreiber2000measuring}, which is equivalent to Granger causality for Gaussian autoregressive processes \cite{barnett2009granger}. Both of these quantities measure the reduction in uncertainty (i.e. conditional entropy) of the future of $X^n$ that is obtained by including $Y^n$ in the available information in an appropriate sense. Interestingly, both quantities are determined by taking expectations over all sequences, and thus are dependent solely on a system's underlying distribution and not a given realization of the collection of processes.

These quantities may be adapted to incorporate a notion of locality through use of \emph{self-information}. For a given realization $x$ of a random variable $X\sim f_X$, the self-information is given by $h(x) \triangleq -\log(f_X(x))$ and represents the amount of surprise associated with that realization. By replacing entropy with self-information, and its conditional form $h(x\mid y) \triangleq -\log(f_{X\mid Y}(x\mid y))$, local versions of DI, TE, and their conditional extensions may be obtained (see Table 1 in \cite{lizier2014jidt} for detailed definitions). While the local extensions of DI and TE are indeed dependent on realizations, they may take on negative values, which is referred to as \emph{anti-causality}. Anti-causality from $Y$ to $X$ occurs when the knowledge that $Y=y$ makes the observation of $X=x$ less likely to have occurred, i.e. $f_X(x) > f_{X\mid Y}(x\mid y)$. While an interesting concept, it is not clear how to interpret anti-causality in the context of assessing the presence/absence of a causal link.

As such, estimating the causal structure (i.e. directed graph) of a collection of processes typically involves estimating the DI or TE between pairs of processes. In settings where the causal structure is presumed to change throughout time, the measure may be calculated using a sliding window in place of an average over all time. As a result, estimates of this form may be able to capture changes in the underlying system model, but do not reflect the varying levels of causal influence that occur even within windows of time for which there is stationarity (see Example \ref{example}). To address this issue, we present a localized measure of causality that is always non-negative, and thus has an unambiguous interpretation in the context of assessing the existence of causal links.

The paper is structured as follows: In Section \ref{measure} we introduce notation and define the causal measure. In Section \ref{estimation} we propose a framework for estimating the causal measure using sequential prediction and derive a finite sample bound on the accuracy of such estimators. In Section \ref{simulations} we demonstrate estimation of the causal measure on simulations. Lastly, Section \ref{discussion} concludes with a discussion of our findings and future work.