\section{Sample Path Measure of Causal Influence}

\begin{comment}
- Definition
- True causal measure is random
- Can be applied to time varying settings
- Always non-negative
- Is always well defined (as opposed to directed information rate)
- ``Semi-local'' - Measure such as DI and TE are functions of the model (can't capture local influence), but local measures like pointwise mutual information can be negative. Our measure is local and positive.
\end{comment}

We begin by defining arbitrary measurable spaces \mcal{X}, \mcal{Y}, and \mcal{Z}. Suppose we observe the stochastic processes $X^n \in \mcal{X}^n$, $Y^n \in \mcal{Y}^n$, and $Z^n \in \mcal{Z}^n$, characterized by the joint probability density function (pdf) $f_{X^n,Y^n,Z^n}(x^n,y^n,z^n)$. We begin by considering the scenario where, having observed $(x^{i-1},y^{i-1},z^{i-1})$, we wish to determine the causal influence that $y^{i-1}$ has the next observation $x^i$. In such a scenario, we consider the following \emph{restricted} (denoted $(r)$) and \emph{complete} (denoted $(c)$) conditional densities:

\begin{eqnarray}
\fxr{i}(x_i) \triangleq f_{X_i \mid X^{i-1},Z^{i-1}}
    (x_i \mid x^{i-1},z^{i-1}) \\
\fxc{i}(x_i) \triangleq f_{X_i \mid X^{i-1},Y^{i-1},Z^{i-1}}
    (x_i \mid x^{i-1},y^{i-1},z^{i-1}).
\end{eqnarray}

\noindent Using these densities, at each time $i$ we define the sample path measure of causality from $Y^n$ to $X^n$ for a given realizations $(x^{i-1},y^{i-1},z^{i-1})$ as:

\begin{equation}
\cyx(x^{i-1},y^{i-1},z^{i-1}) = \kl{\fxc{i}}{\fxr{i}}.
\end{equation}

\noindent For ease of notation, we may represent the causal measure at time $i$ simply as $\cyx(i)$.

The key observation that must be made that \fxc{i} and \fxr{i} are determined by the realizations of $X^n$, $Y^n$, and $Z^n$. As a result, \emph{the causal measure is a random variable}. In this regard, our causal measure is different from previous measures of causality wherein the causal influence is determined by the model, and not the sample path. To ensure this point is made clear, we will present an example.

\begin{example}
Suppose $Y_i \sim \text{Bern}(0.2)$ iid for $i=1,2,\dots$ and:

\begin{equation}
X_i \sim
\begin{cases}
      \text{Bern}(0.9), & Y_{i-1} = 1 \\
      \text{Bern}(0.5), & Y_{i-1} = 0
\end{cases}
\end{equation}

\noindent Intuitively, we would expect that in some sense $Y^n$ is ``causing'' $X^n$ to a greater extent when $Y_i$ is one than when it is zero. In order formalize this, we have to find the probability of $X_i=1$ when only $X^{i-1}$ is known:

\begin{equation*}
\begin{aligned}
\mathbb{P}(&X_i = 1 | X^{i-1}=x^{i-1}) \\
&= \mathbb{P}(X_i = 1) \\
&= \sum_{y_{i-1}\in \{0,1\}}
    \mathbb{P}(X_i =1 \mid Y_{i-1} = y_{i-1}) \mathbb{P}(Y_{i-1} = y_{i-1}) \\
&= (0.5)(0.8) + (0.9)(0.2) \\
&= 0.58.
\end{aligned}
\end{equation*}

\noindent We can fully characterize the complete and restricted probability mass functions (pmfs) using these probabilities, i.e. $\fxr{i}(1) = 0.58$, $\fxc{i}(1) = 0.9$ if $y_{n-1}=1$, and $\fxc{i}(1) = 0.5$ if $y_{n-1}=0$. We can now compute the causal measure, which takes on one of two values determined by the observation $y_{i-1}$:

\begin{equation}
\cyx(i) =
\begin{cases}
      0.363, & y_{i-1} = 1 \\
      0.019, & y_{i-1} = 0
\end{cases}
\end{equation}

\noindent Thus, we see that our measure captures how, even in a stationary Markov chain, different patterns in the observed data may give rise to different levels of causal influence. By contrast, we note that because the process is stationary, the directed information rate and transfer entropy are both given simply by $E[\cyx]=(0.9)(0.019)+(0.1)(0.363)=0.088$.
\end{example}

The above example gives rise to two key observations. First, transfer entropy and directed information rate fail to capture that the causal effect of $\{Y\}$ upon $\{X\}$ varies in time, even in the simplest of stationary processes. Second, by averaging over all possible histories, transfer entropy and directed information rate are minimally affected by patterns that occur with low probability, even if those patterns induce a high level of causal influence.

We now discuss some key properties of the proposed causal measure. First, we note the crucially important quality of non-negativity, which follows directly from the fact that the measure is nothing more than a KL-Divergence. Next, we characterize our measure as being ``semi-local.'' This term is meant to illustrate that our measure is in a sense between existing measures or causality. In particular, we note that the directed information (rate), Granger causality, transfer entropy, etc. are all \emph{expectations}, determined entirely by the underlying probabilistic model of the observed data. On the other end of the spectrum, local data-dependent versions of these measures may be obtained by substituting the self information for entropy. Unfortunately, these local versions of causal measures may negative when unlikely sequences occur. Our measure can be thought of as being determined by the underlying statistical model, but the parameters of the model are determined by the past data. We use the term ``semi-local'' because at any given time, the measure is determined by the observations from the past and an expectation over the future.





\begin{comment}
\noindent This process can be equivalently characterized by the four-state ``complete'' Markov Chain with states $(X,Y) \in \{0,1\}^2$ and transition matrix:

\begin{equation}
M^{(c)} =
\begin{blockarray}{ccccc}
(0,0) & (0,1) & (1,0) & (1,1) \\
\begin{block}{(cccc)c}
    0.25 & 0.1 & 0.25 & 0.1 & (0,0) \\
    0.25 & 0.9 & 0.25 & 0.9 & (0,1) \\
    0.25 & 0.1 & 0.25 & 0.1 & (1,0) \\
    0.25 & 0.9 & 0.25 & 0.9 & (1,1) \\
\end{block}
\end{blockarray}
\end{equation}

\noindent where $M^{(c)}_{kj}$ represents the probability of having $(X_i,Y_i)$ in state $k$ given that $(X_{i-1},Y_{i-1})$ is in state $j$. We can additionally define a two-state ``restricted'' Markov Chain with states $X \in \{0,1\}$ and transition matrix:

\begin{equation}
M^{(r)} =
\begin{blockarray}{ccc}
0 & 1 \\
\begin{block}{(cc)c}
    0.3 & 0.3 & 0 \\
    0.7 & 0.7 & 1 \\
\end{block}
\end{blockarray}
\end{equation}
\end{comment}